# Null-layer

Code for study of techinique null-layer on convolution Neural Networks (CNN).<br />
Three different networks, four datasets, five activation functions and three types of weight initialization were used.<br />

Initializations: Glorot, He and Narrow-normal. <br />
Activation functions: Exponential Linear Unit (ELU), Rectified Linear Unit (ReLU), Leaky Rectified Linear Unit (LReLU), Clipped Rectified Linear Unit (CReLU) and Tanh. <br />
Architectures: AlexNet, VGG19 and a three convolutional layer architecture. <br />
Datasets: [Cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html), [MNIST](http://yann.lecun.com/exdb/mnist/), [Flower-recognition](https://www.kaggle.com/alxmamaev/flowers-recognition) and [fruit-recognition](https://www.kaggle.com/sshikamaru/fruit-recognition).<br /><br />


# How to to run the code ?

1- Make sure you have the datasets in your directory. <br />
2- Download the script null-layer.<br />
3- Change the paths in scripts.<br />
4- Choose the dataset.<br />
5- Set up the proportions on datasets.<br />
6- Pick up the apropriate layer in script named layers.<br />

