# Null-layer

Code for study of techinique null-layer on convolution Neural Networks (CNN).<br />
Three different networks, four datasets, five activation functions and three types of weight initialization were used.<br />

Initializations: Glorot, He and Narrow-normal; <br />
Activation functions: Exponential Linear Unit (ELU), Rectified Linear Unit (ReLU), Leaky Rectified Linear Unit (LReLU), Clipped Rectified Linear Unit (CReLU) and Tanh; three Architectures: AlexNet, VGG19 and a three convolutional layer architecture; <br />
Datasets. <br />
