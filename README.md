# Null-layer

Code for study of techinique null-layer on convolution Neural Networks (CNN).<br />
Three different networks, four datasets, five activation functions and three types of weight initialization were used.<br />

Initializations: Glorot, He and Narrow-normal; <br />
Activation functions: Exponential Linear Unit (ELU), Rectified Linear Unit (ReLU), Leaky Rectified Linear Unit (LReLU), Clipped Rectified Linear Unit (CReLU) and Tanh; three Architectures: AlexNet, VGG19 and a three convolutional layer architecture; <br />
Datasets. [Cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html), [MNIST](http://yann.lecun.com/exdb/mnist/), [Flower-recognition](https://www.kaggle.com/alxmamaev/flowers-recognition) and [fruit-recognition](https://www.kaggle.com/sshikamaru/fruit-recognition).<br />
