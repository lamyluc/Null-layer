# Null-layer

Code for study of techinique null -layer on convolution Neural Networks (CNN).<br />
Three different networks, four datasets, five activation functions and three types of weight initialization were used.<br />

Initializations were used: Glorot, He and Narrow-normal; <br />
five activation functions (AF): Exponential Linear Unit (ELU), Rectified Linear Unit (ReLU), Leaky Rectified Linear Unit (LReLU), Clipped Rectified Linear Unit (CReLU) and Tanh; three distinct architectures: AlexNet, VGG19 and a three convolutional layer architecture; <br />
four datasets. <br />
